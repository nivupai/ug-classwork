{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_initial_data(input_path):\n",
    "    preprocessed_data = []\n",
    "    with open(input_path, \"r\") as content:\n",
    "        for line in content.readlines():\n",
    "            words = line.strip().split()  # split the line into words\n",
    "            if words and words[-1][-1] in ('.', '?', '!'):  # check if the last word has punctuation\n",
    "                words[-1] = words[-1][:-1]  # remove punctuation from the last word\n",
    "            words = ['<s>'] + words + ['</s>'] if words else []  # add start and end markers for bigram\n",
    "            preprocessed_data.append(words)  # save the preprocessed line to the list\n",
    "    return preprocessed_data\n",
    "\n",
    "# input paths\n",
    "english = \"../Data/Input/LangId.train.English\"\n",
    "french = \"../Data/Input/LangId.train.French\"\n",
    "italian = \"../Data/Input/LangId.train.Italian\"\n",
    "\n",
    "# preprocess english, french, italian data\n",
    "english_pre = preprocess_initial_data(english)\n",
    "french_pre = preprocess_initial_data(french)\n",
    "italian_pre = preprocess_initial_data(italian)\n",
    "\n",
    "def preprocess_test_data(input_path):\n",
    "    preprocessed_data = []\n",
    "    with open(input_path, \"r\") as content:\n",
    "        for line in content.readlines():\n",
    "            words = line.strip().split()  # split the line into words\n",
    "            if words and words[-1][-1] in ('.', '?', '!'):  # check if the last word has punctuation\n",
    "                words[-1] = words[-1][:-1]  # remove punctuation from the last word\n",
    "            words = ['<s>'] + words + ['</s>']  # add start and end markers for bigram\n",
    "            preprocessed_data.append(words)  # save the preprocessed line to the list\n",
    "    return preprocessed_data\n",
    "\n",
    "test_path = \"../Data/Validation/LangId.test\"\n",
    "\n",
    "# preprocess test data\n",
    "test_preprocessed = preprocess_test_data(test_path)\n",
    "\n",
    "def create_bigram(data):\n",
    "    bigram = {}\n",
    "\n",
    "    for sentence in data:  # iterate through each sentence in the data\n",
    "        for index in range(len(sentence) - 1):  # iterate through each word in the sentence\n",
    "            current_word = sentence[index]\n",
    "            next_word = sentence[index + 1]\n",
    "\n",
    "            # check if the current word is already in the bigram dictionary\n",
    "            if current_word in bigram:\n",
    "                # check if the next word is already in the current word's dictionary\n",
    "                if next_word in bigram[current_word]:\n",
    "                    # increment the count for the next word in the current word's dictionary\n",
    "                    bigram[current_word][next_word] += 1\n",
    "                else:\n",
    "                    # add the next word to the current word's dictionary and initialize its count to 1\n",
    "                    bigram[current_word][next_word] = 1\n",
    "            else:\n",
    "                # add the current word to the bigram dictionary with the next word as a key and count as 1\n",
    "                bigram[current_word] = {next_word: 1}\n",
    "    return bigram\n",
    "\n",
    "english_bigram = create_bigram(english_pre)\n",
    "french_bigram = create_bigram(french_pre)\n",
    "italian_bigram = create_bigram(italian_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log, exp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def count_frequencies(data):\n",
    "    count_dict = {}\n",
    "    counts = []\n",
    "    frequencies = []\n",
    "\n",
    "    # calculate counts and frequencies\n",
    "    for key in data:\n",
    "        for sub_key in data[key]:\n",
    "            count = data[key][sub_key]\n",
    "\n",
    "            count_dict[count] = count_dict.get(count, 0) + 1\n",
    "\n",
    "            # update counts and frequencies lists\n",
    "            if count not in counts:\n",
    "                counts.append(count)\n",
    "                frequencies.append(1)\n",
    "            else:\n",
    "                frequencies[counts.index(count)] += 1\n",
    "\n",
    "    return count_dict, counts, frequencies\n",
    "\n",
    "def transform_data(counts, frequencies):\n",
    "    # transform frequencies for regression\n",
    "    log_frequencies = [log(y) for y in frequencies]\n",
    "    counts = np.array(counts).reshape(-1, 1)\n",
    "    log_frequencies = np.array(log_frequencies)\n",
    "\n",
    "    return counts, log_frequencies\n",
    "\n",
    "def smooth_model_data(data, count_dict, counts, frequencies):\n",
    "    # fit linear regression model\n",
    "    regression_model = LinearRegression().fit(counts, frequencies)\n",
    "\n",
    "    # smooth the data using regression coefficients\n",
    "    for key in data:\n",
    "        for sub_key in data[key]:\n",
    "            count = data[key][sub_key]\n",
    "\n",
    "            if count + 1 in count_dict and count in count_dict:\n",
    "                data[key][sub_key] = (count + 1) * (count_dict[count + 1]) / count_dict[count]\n",
    "            else:\n",
    "                data[key][sub_key] = (count + 1) * (exp(regression_model.predict([[count + 1], ]))) / count_dict[count]\n",
    "\n",
    "    return data\n",
    "\n",
    "def estimate_unseen_frequency(count_dict, model_size):\n",
    "    # estimate unseen frequency (N1/N)\n",
    "    return count_dict[1] / (model_size * model_size - sum(frequencies))\n",
    "\n",
    "def smooth_gt_model(data):\n",
    "    count_dict, counts, frequencies = count_frequencies(data)\n",
    "    counts, log_frequencies = transform_data(counts, frequencies)\n",
    "    data = smooth_model_data(data, count_dict, counts, frequencies)\n",
    "    data[\"unseen_frequency\"] = estimate_unseen_frequency(count_dict, len(data))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_path = \"../Data/Output/wordLangId2.out\"\n",
    "\n",
    "with open(result_file_path, \"r\") as result_file:\n",
    "    result_list = [line.split()[1] for line in result_file.readlines()]\n",
    "\n",
    "answer_file_path = \"../Data/Validation/labels.sol\"\n",
    "\n",
    "with open(answer_file_path, \"r\") as answer_file:\n",
    "    answer_list = [line.split()[1] for line in answer_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\"actual\": result_list, \"expected\":answer_list}\n",
    "df = pd.DataFrame(data)\n",
    "def check_similarity(actual, expected):\n",
    "    return 1 if actual == expected else 0\n",
    "\n",
    "df[\"similarity_score\"] = df.apply(lambda x: check_similarity(x['actual'], x['expected']), axis=1)\n",
    "accuracy = sum(df[\"similarity_score\"]) / len(df)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
