{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    words = text.split()  # split the text into words\n",
    "    if words and words[-1][-1] in ('.', '?', '!'):  # check if the last word has punctuation\n",
    "        words[-1] = words[-1][:-1]  # remove punctuation from the last word\n",
    "    return ['<s>'] + words + ['</s>'] if words else []  # add start and end markers for bigram\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    preprocessed_data = []\n",
    "    with open(file_path, \"r\") as content:\n",
    "        for line in content.readlines():\n",
    "            preprocessed_line = preprocess_sentence(line)\n",
    "            if preprocessed_line:  # check if the preprocessed line is not empty\n",
    "                preprocessed_data.append(preprocessed_line)\n",
    "    return preprocessed_data\n",
    "\n",
    "eng_train_path = \"../Data/Input/LangId.train.English\"\n",
    "eng_preprocessed = load_and_preprocess_data(eng_train_path)\n",
    "\n",
    "fre_train_path = \"../Data/Input/LangId.train.French\"\n",
    "fre_preprocessed = load_and_preprocess_data(fre_train_path)\n",
    "\n",
    "it_train_path = \"../Data/Input/LangId.train.Italian\"\n",
    "it_preprocessed = load_and_preprocess_data(it_train_path)\n",
    "\n",
    "test_data_path = \"../Data/Validation/LangId.test\"\n",
    "with open(test_data_path, \"r\") as test_content:\n",
    "    test_preprocessed = []\n",
    "    for line in test_content.readlines():\n",
    "        words = line.strip().split()\n",
    "        # remove punctuation at the end of the sentence\n",
    "        if words[-1] in ('.', '?', '!'):\n",
    "            words = words[:-1]\n",
    "        # Add start and end markers for bigram\n",
    "        words = ['<s>'] + words + ['</s>']\n",
    "        test_preprocessed.append(words)\n",
    "\n",
    "def generate_bigram(data):\n",
    "    bigram = {}\n",
    "    for sentence in data:  # iterate over each sentence in the data\n",
    "        for idx in range(len(sentence) - 1):  # iterate over each word index in the sentence\n",
    "            current_word = sentence[idx]\n",
    "            next_word = sentence[idx + 1]\n",
    "            # initialize the bigram dictionary for the current word if it doesn't exist\n",
    "            if current_word not in bigram:\n",
    "                bigram[current_word] = {}\n",
    "            # increment the count of the next word following the current word\n",
    "            if next_word in bigram[current_word]:\n",
    "                bigram[current_word][next_word] += 1\n",
    "            else:\n",
    "                bigram[current_word][next_word] = 1\n",
    "    return bigram\n",
    "\n",
    "eng_bigram = generate_bigram(eng_preprocessed)\n",
    "fre_bigram = generate_bigram(fre_preprocessed)\n",
    "it_bigram = generate_bigram(it_preprocessed)\n",
    "\n",
    "def calculate_probability(model, word_prev, word_n):\n",
    "    count = 0  \n",
    "    total = 0  \n",
    "    if word_prev in model:\n",
    "        if word_n in model[word_prev]:\n",
    "            count = model[word_prev][word_n]\n",
    "        total = sum(model[word_prev].values())\n",
    "        # calculate the conditional probability P(word_n | word_prev)\n",
    "        if total > 0:\n",
    "            result = count / total\n",
    "        else:\n",
    "            result = 0.0  # handle division by zero\n",
    "    else:\n",
    "        result = 0.0  # if word_prev is not in the model, return probability 0\n",
    "    return result\n",
    "\n",
    "result_file_path = \"../Data/Output/wordLangId.out\"\n",
    "\n",
    "def read_language_predictions(file_path):\n",
    "    result_list = []\n",
    "    with open(file_path, \"r\") as result_file:\n",
    "        for line in result_file.readlines():\n",
    "            lang = line.strip().split()[1]\n",
    "            result_list.append(lang)\n",
    "    return result_list\n",
    "\n",
    "result_list = read_language_predictions(result_file_path)\n",
    "\n",
    "actual_file_path = \"../Data/Validation/labels.sol\"\n",
    "\n",
    "def read_actual_labels(file_path):\n",
    "    actual_list = []\n",
    "    with open(file_path, \"r\") as ans_file:\n",
    "        for line in ans_file.readlines():\n",
    "            label = line.strip().split()[1]\n",
    "            actual_list.append(label)\n",
    "    return actual_list\n",
    "\n",
    "actual_list = read_actual_labels(actual_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433333333333334"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(model_list, actual_list):\n",
    "    data = {\"actual\": model_list, \"expected\": actual_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = create_dataframe(result_list, actual_list)\n",
    "\n",
    "def check_similarity(actual, expected):\n",
    "    return 1 if actual == expected else 0\n",
    "\n",
    "df[\"similarity_score\"] = df.apply(lambda x: check_similarity(x['actual'], x['expected']), axis=1)\n",
    "accuracy = sum(df[\"similarity_score\"]) / len(df)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
